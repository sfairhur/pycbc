#!/usr/bin/env python
import h5py, argparse, logging, numpy, numpy.random
from pycbc import events, detector
from pycbc.events import veto, coinc, stat
import pycbc.version
import lal
from pycbc.coherent import likelihood, fstat, sky
from scipy.misc import logsumexp
       
parser = argparse.ArgumentParser()
parser.add_argument("--verbose", action="count")
parser.add_argument("--version", action="version", version=pycbc.version.git_verbose_msg)
parser.add_argument("--veto-files", nargs='*', action='append', default=[],
                    help="Optional veto file. Triggers within veto segments "
                    "contained in the file are ignored")
parser.add_argument("--segment-name", nargs='*', action='append', default=[],
                    help="Optional, name of veto segment in veto file")
parser.add_argument("--strict-coinc-time", action='store_true',
                    help="Optional, only allow coincidences between triggers "
                    "that lie in coincident time after applying vetoes")
parser.add_argument("--trigger-files", nargs=2,
                    help="File containing the single-detector triggers")
parser.add_argument("--template-bank", required=True,
                    help="Template bank file in HDF format")
# produces a list of lists to allow multiple invocations and multiple args
parser.add_argument("--statistic-files", nargs='*', action='append', default=[],
                    help="Files containing ranking statistic info")
parser.add_argument("--ranking-statistic", choices=stat.statistic_dict.keys(),
                    help="The ranking statistic to use", default='newsnr')
parser.add_argument("--coinc-threshold", type=float, default=0.0,
                    help="Seconds to add to time-of-flight coincidence window")
parser.add_argument("--timeslide-interval", type=float,
                    help="Interval between timeslides in seconds. Timeslides are "
                         "disabled if the option is omitted.")
parser.add_argument("--newsnr-threshold", type=float, default=0.0,
                    help="Only keep events with a newsnr greater than this value")
parser.add_argument("--decimation-factor", type=int,
                    help="The factor to reduce the background trigger rate.")
loudest = parser.add_mutually_exclusive_group()
loudest.add_argument("--loudest-keep", type=int,
                     help="Keep this number of loudest triggers from each template.")
loudest.add_argument("--loudest-keep-value", type=float,
                     help="Keep all coincident triggers above this value.")
parser.add_argument("--template-fraction-range", default="0/1",
                    help="Optional, analyze only part of template bank. Format"
                    " PART/NUM_PARTS")
parser.add_argument("--cluster-window", type=float,
                    help="Optional, window size in seconds to cluster "
                    "coincidences over the bank")
parser.add_argument("--output-file",
                    help="File to store the coincident triggers")
args = parser.parse_args()

# flatten the list of lists of filenames to a single list (may be empty)
args.statistic_files = sum(args.statistic_files, [])
args.segment_name = sum(args.segment_name, [])
args.veto_files = sum(args.veto_files, [])

if args.verbose:
    logging.basicConfig(format='%(asctime)s : %(message)s', level=logging.DEBUG)

def get_statistic(option, files):
    if option == 'newsnr':
        return NewSNRStatistic(files)
    elif option == 'newsnr_cut':
        return NewSNRCutStatistic(files)
    elif option == 'phasetd_newsnr':
        return PhaseTDStatistic(files)
    elif option == 'likelihood_newsnr':
        return LikelihoodStatistic(files)
    elif option == 'max_cont_trad_newsnr':
        return MaxContTradNewSNRStatistic(files)
    else:
        raise ValueError('%s is not an available detection statistic' % option)

def parse_template_range(num_templates, rangestr):
    part = int(rangestr.split('/')[0])
    pieces = int(rangestr.split('/')[1])
    tmin =  int(num_templates / float(pieces) * part)
    tmax =  int(num_templates / float(pieces) * (part+1))
    return tmin, tmax    

def generate_sky_rings(dt_points=180, ring_points=300):
    t_gps = 1000000000.0
    t_gmst = lal.GreenwichMeanSiderealTime(t_gps)
    sky_line = sky.MaxTimeDelayLine3("H1", "L1", 0.9, 0.0, t_gps, n=dt_points)
    dt_ring = numpy.array([y.get_time_delay("H1", "L1", t_gps) for y in sky_line])
    f_plus_cross = []
    for i, pt in enumerate(sky_line):
        dt = pt.get_time_delay("H1", "L1", t_gps)
        ring = sky.ISOTimeDelayLine(["H1", "L1"], pt.longitude, pt.latitude, 
                                    t_gps, ring_points)
        f_pc = numpy.zeros([len(ring),2,2])
        for j, r in enumerate(ring):
            f_pc[j] = numpy.array([numpy.array(lal.ComputeDetAMResponse(
                detector.Detector(ifo).response,
                r.longitude, r.latitude, 0., t_gmst))
                for ifo in ["H1", "L1"]])
        f_plus_cross.append(f_pc)
    return dt_ring, f_plus_cross

class Stat(object):
    def __init__(self, files):
        self.files = {}
        for filename in files:
            f = h5py.File(filename, 'r')
            stat = f.attrs['stat']
            self.files[stat] = f

class NewSNRStatistic(Stat):
    def single(self, trigs):
        """ Read in the single detector information and make a single detector
        statistic. Results can either be a single number or a record array.
        """
        
        dof = 2 * trigs['chisq_dof'] - 2
        newsnr = events.newsnr(trigs['snr'], trigs['chisq'] / dof)
        return numpy.array(newsnr, ndmin=1, dtype=numpy.float32)

    def coinc(self, s1, s2, slide, step):
        """ Calculate the coincident statistic.
        """
        return (s1**2.0 + s2**2.0) ** 0.5

class NewSNRCutStatistic(Stat):
    def single(self, trigs):
        dof = 2 * trigs['chisq_dof'] - 2
        rchisq = trigs['chisq'] / dof
        newsnr = events.newsnr(trigs['snr'], rchisq)
        newsnr[numpy.logical_and(newsnr < 10, rchisq > 2)] = -1
        return newsnr

    def coinc(self, s1, s2, slide, step):
        cstat = (s1**2.0 + s2**2.0) ** 0.5
        cstat[s1==-1] = 0
        cstat[s2==-1] = 0
        return cstat

class PhaseTDStatistic(NewSNRStatistic):
    def single(self, trigs):
        newsnr = NewSNRStatistic.single(self, trigs)
        return numpy.array((newsnr, trigs['coa_phase'], trigs['end_time'])).transpose()

    def coinc(self, s1, s2, slide, step):
        """ Calculate the coincident statistic.
        """
        td = s1[:,2] - s2[:,2] - slide * step
        pd = s1[:,1] - s2[:,1]
        
        tbins = self.files['phasetd_newsnr']['tbins'][:]
        pbins = self.files['phasetd_newsnr']['pbins'][:]
        
        tv = numpy.searchsorted(tbins, td) - 1 
        pv = numpy.searchsorted(pbins, pd) - 1
        
        tv[tv < 0] = 0
        tv[tv >= len(tbins) - 1] = len(tbins) - 2
        pv[pv < 0] = 0
        pv[pv >= len(pbins) - 1] = len(pbins) - 2
        
        m = self.files['phasetd_newsnr']['map'][:][tv, pv]
        m[m < 1] = 1
        
        cstat = (s1[:,0]**2.0 + s2[:,0]**2.0 + 2.0 * numpy.log(m))**0.5  
         
        return cstat

class LikelihoodStatistic(NewSNRStatistic):
    def single(self, trigs):
        newsnr = NewSNRStatistic.single(self, trigs)
        return numpy.array((newsnr, trigs['end_time'],
             trigs['snr'], trigs['coa_phase'], 
             numpy.sqrt(trigs['sigmasq']))).transpose()

    def coinc(self, s1, s2, slide, step):
        """ Calculate the coincident statistic.
        """
        td = s1[:,1] - s2[:,1] - slide * step
        z = numpy.array([s1[:,2] * numpy.exp(1j * s1[:,3]), 
            s2[:,2] * numpy.exp(1j * s2[:,3])])
        sigma = numpy.array([s1[:,4], s2[:,4]])
        dmax = numpy.linalg.norm(sigma)/8/numpy.sqrt(2)
        cstat = numpy.zeros_like(td)
        snr = numpy.linalg.norm(z, axis=0)
        newsnrsq = s1[:,0]**2 + s2[:,0]**2
        loud = numpy.sqrt(newsnrsq) > args.newsnr_threshold
        logging.info('Coincident Trigs above threshold: %s' % (sum(loud)))

        
        for c in loud.nonzero()[0]:
            i = (abs(dt_ring - td[c])).argmin()
            f_pc = f_plus_cross[i]
            lik = numpy.zeros(len(f_pc))
            for j, f in enumerate(f_pc):
                f_sig = f * sigma[:,c]
                a = fstat.snr_f_to_a(z[:,c], f_sig)
                fp, fc, chi = fstat.dominant_polarization(f_sig)
                D, cosi, psi, phi = fstat.a_to_params(a)
                a_dp = fstat.params_to_a(D, cosi, psi - chi, phi)
                lik[j] = likelihood.like_approx(a_dp, fp, fc, dmax)[0]
            cstat[c] = logsumexp(lik) - numpy.log(len(f_pc)) - snr[c]**2/2 + newsnrsq[c]/2
        return cstat 
        

class MaxContTradNewSNRStatistic(NewSNRStatistic):
    def single(self, trigs):
        """Combined chisq calculation for each trigger."""
        chisq_dof = 2 * trigs['chisq_dof'] - 2
        chisq_newsnr = events.newsnr(trigs['snr'], trigs['chisq'] / chisq_dof)
        autochisq_dof = trigs['cont_chisq_dof']
        autochisq_newsnr = events.newsnr(trigs['snr'],
                                         trigs['cont_chisq'] / autochisq_dof)
        return numpy.array(numpy.minimum(chisq_newsnr, autochisq_newsnr,
                             dtype=numpy.float32), ndmin=1, copy=False)

class ReadByTemplate(object):
    def __init__(self, filename, bank=None, segment_name=[], veto_files=[]):
        self.filename = filename
        self.file = h5py.File(filename, 'r')
        self.ifo = self.file.keys()[0]
        self.valid = None
        self.bank = h5py.File(bank) if bank else None

        # Determine the segments which define the boundaries of valid times
        # to use triggers
        from pycbc_glue.segments import segmentlist, segment
        key = '%s/search/' % self.ifo
        s, e = self.file[key + 'start_time'][:], self.file[key + 'end_time'][:]
        self.segs = veto.start_end_to_segments(s, e).coalesce()
        for vfile, name in zip(veto_files, segment_name):
            veto_segs = veto.select_segments_by_definer(vfile, ifo=self.ifo,
                                                     segment_name=name)
            self.segs = (self.segs - veto_segs).coalesce()
        self.valid = veto.segments_to_start_end(self.segs)

    def get_data(self, col, num):
        """ Get a column of data for template with id 'num'

        Parameters
        ----------
        col: str
            Name of column to read
        num: int
            The template id to read triggers for

        Returns
        -------
        data: numpy.ndarray
            The requested column of data
        """
        ref = self.file['%s/%s_template' % (self.ifo, col)][num]
        return self.file['%s/%s' % (self.ifo, col)][ref]

    def set_template(self, num):
        """ Set the active template to read from

        Parameters
        ----------
        num: int
            The template id to read triggers for

        Returns
        -------
        trigger_id: numpy.ndarray
            The indices of this templates triggers
        """
        self.template_num = num
        times = self.get_data('end_time', num)

        # Determine which of these template's triggers are kept after
        # applying vetoes
        if self.valid:
            self.keep = veto.indices_within_times(times, self.valid[0], self.valid[1])
            logging.info('applying vetoes')
        else:
            self.keep = numpy.arange(0, len(times))

        if self.bank is not None:
            self.param = {}
            if 'parameters' in self.bank.attrs :
                for col in self.bank.attrs['parameters']:
                    self.param[col] = self.bank[col][self.template_num]
            else :
                for col in self.bank:
                    self.param[col] = self.bank[col][self.template_num]

        # Calculate the trigger id by adding the relative offset in self.keep
        # to the absolute beginning index of this templates triggers stored
        # in 'template_boundaries'
        trigger_id = self.keep + self.file['%s/template_boundaries' % self.ifo][num]
        return trigger_id

    def __getitem__(self, col):
        """ Return the column of data for current active template after
        applying vetoes

        Parameters
        ----------
        col: str
            Name of column to read

        Returns
        -------
        data: numpy.ndarray
            The requested column of data
        """
        if self.template_num == None:
            raise ValueError('You must call set_template to first pick the '
                             'template to read data from')
        data = self.get_data(col, self.template_num)
        data = data[self.keep] if self.valid else data
        return data

logging.info('Starting...')

num_templates = len(h5py.File(args.template_bank, "r")['template_hash'])
tmin, tmax = parse_template_range(num_templates, args.template_fraction_range)
logging.info('Analyzing template %s - %s' % (tmin, tmax-1))

logging.info('Opening first trigger file: %s' % args.trigger_files[0])
trigs0= ReadByTemplate(args.trigger_files[0],
                       args.template_bank, args.segment_name, args.veto_files)
logging.info('Opening second trigger file: %s' % args.trigger_files[1])
trigs1 = ReadByTemplate(args.trigger_files[1],
                        args.template_bank, args.segment_name, args.veto_files)
coinc_segs = (trigs0.segs & trigs1.segs).coalesce()

if args.strict_coinc_time:
    trigs0.segs = coinc_segs
    trigs1.segs = coinc_segs
    trigs0.valid = veto.segments_to_start_end(trigs0.segs)
    trigs1.valid = veto.segments_to_start_end(trigs1.segs)

if args.ranking_statistic == "likelihood_newsnr":
    dt_ring, f_plus_cross = generate_sky_rings()
 
# initialize a Stat class instance to calculate the coinc ranking statistic
rank_method = stat.get_statistic(args.ranking_statistic)(args.statistic_files)
det0, det1 = detector.Detector(trigs0.ifo), detector.Detector(trigs1.ifo)
time_window = det0.light_travel_time_to_detector(det1) + args.coinc_threshold

if time_window >= args.timeslide_interval and args.timeslide_interval is not None:
    raise parser.error("The maximum time delay between detectors should be smaller "
                       "than the timeslide interval.")

# slide = 0 significes don't do timeslides
if args.timeslide_interval is None:
    args.timeslide_interval = 0

logging.info('The coincidence window is %3.1f ms' % (time_window * 1000))

data = {'stat':[], 'decimation_factor':[], 'time1':[], 'time2':[],
        'trigger_id1':[], 'trigger_id2':[], 'timeslide_id':[], 'template_id':[]}

for tnum in range(tmin, tmax):
    tid0 = trigs0.set_template(tnum)
    tid1 = trigs1.set_template(tnum)

    if (len(tid0) == 0) or (len(tid1) == 0):
        continue

    t0 = trigs0['end_time']
    t1 = trigs1['end_time']
    logging.info('Trigs for template %s, %s:%s %s:%s' % \
                (tnum, trigs0.ifo, len(t0), trigs1.ifo, len(t1)))

    i0, i1, slide = coinc.time_coincidence(t0, t1, time_window, args.timeslide_interval)

    logging.info('Coincident Trigs: %s' % (len(i1)))

    logging.info('Calculating Single Detector Statistic')
    s0, s1 = rank_method.single(trigs0), rank_method.single(trigs1)

    logging.info('Calculating Multi-Detector Combined Statistic')
    c = rank_method.coinc(s0[i0], s1[i1], slide, args.timeslide_interval)

    #index values of the zerolag triggers
    fi = numpy.where(slide == 0)[0]

    #index values of the background triggers
    bi = numpy.where(slide != 0)[0]
    logging.info('%s foreground triggers' % len(fi))
    logging.info('%s background triggers' % len(bi))

    # We split the background triggers into two types which we keep track of
    # in "bh" (triggers which are *not* decimated, stored in full) and
    # "bl" (triggers which may not be stored in full, but we keep track of
    # how many are removed)
    # "bl_int" keeps track of which triggers are *not* in the bh set. Depending
    # on the decimation factor option we may not store any of these or we may
    # keep a fraction of them corresponding to a subset of the timeslides
    if args.loudest_keep:
        sep = len(bi) - args.loudest_keep
        sep = 0 if sep < 0 else sep

        bsort = numpy.argpartition(c[bi], sep)
        bl_int = bi[bsort[0:sep]]
        bh = bi[bsort[sep:]]
        del bsort
        del bi
    elif args.loudest_keep_value:
        bh = bi[c[bi] > args.loudest_keep_value]
        bl_int = bi[c[bi] <= args.loudest_keep_value]
    else:
        bh = bi

    if args.decimation_factor:
        bl = bl_int[slide[bl_int] % args.decimation_factor == 0]
    else:
        bl = []

    ti = numpy.concatenate([bl, bh, fi]).astype(numpy.uint32)
    logging.info('%s after decimation' % len(ti))

    g0 = i0[ti]
    g1 = i1[ti]
    del i0
    del i1

    data['stat'] += [c[ti]]
    dec_fac = numpy.repeat([args.decimation_factor, 1, 1],
                           [len(bl), len(bh), len(fi)]).astype(numpy.uint32)
    data['decimation_factor'] += [dec_fac]
    data['time1'] += [t0[g0]]
    data['time2'] += [t1[g1]]
    data['trigger_id1'] += [tid0[g0]]
    data['trigger_id2'] += [tid1[g1]]
    data['timeslide_id'] += [slide[ti]]
    data['template_id'] += [numpy.zeros(len(ti), dtype=numpy.uint32) + tnum]

if len(data['stat']) > 0:
    for key in data:
        data[key] = numpy.concatenate(data[key])

if args.cluster_window and len(data['stat']) > 0:
    cid = coinc.cluster_coincs(data['stat'], data['time1'], data['time2'],
                               data['timeslide_id'], args.timeslide_interval,
                               args.cluster_window)

logging.info('saving coincident triggers')
f = h5py.File(args.output_file, 'w')
if len(data['stat']) > 0:
    for key in data:
        var = data[key][cid] if args.cluster_window else data[key]
        f.create_dataset(key, data=var,
                              compression='gzip',
                              compression_opts=9,
                              shuffle=True)

f['segments/coinc/start'], f['segments/coinc/end'] = veto.segments_to_start_end(coinc_segs)

for t in [trigs0, trigs1]:
    f['segments/%s/start' % t.ifo], f['segments/%s/end' % t.ifo] = t.valid

f.attrs['timeslide_interval'] = args.timeslide_interval
f.attrs['detector_1'] = det0.name
f.attrs['detector_2'] = det1.name
f.attrs['foreground_time1'] = abs(trigs0.segs)
f.attrs['foreground_time2'] = abs(trigs1.segs)
f.attrs['coinc_time'] = abs(coinc_segs)

if args.timeslide_interval:
    nslides = int(max(abs(trigs0.segs), abs(trigs1.segs)) / args.timeslide_interval)
else:
    nslides = 0

f.attrs['num_slides'] = nslides
logging.info('Done')
